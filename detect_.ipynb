{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20161699\\Anaconda3\\envs\\joowan\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np           \n",
    "import xml.etree.ElementTree as Et\n",
    "from PIL import Image as im\n",
    "import math \n",
    "from sklearn.linear_model import Ridge\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets,transforms,models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as im\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# obj_class =[]\n",
    "# for e,i in enumerate(os.listdir( annot_path)):\n",
    "#     xml =  open(os.path.join(annot_path, i), \"r\")\n",
    "#     tree = Et.parse(xml)\n",
    "#     root = tree.getroot()\n",
    "#     objects = root.findall(\"object\")\n",
    "#     obj_class = obj_class + [_object.find('name').text for _object in objects ]\n",
    "#     obj_class = list(set(obj_class))\n",
    "#     if len(obj_class) == 20:\n",
    "#         break\n",
    "\n",
    "\n",
    "obj_class = ['person', \n",
    "           'bird', \n",
    "           'cat', \n",
    "           'cow', \n",
    "           'dog',    # 5\n",
    "           'horse', \n",
    "           'sheep', \n",
    "           'aeroplane', \n",
    "           'bicycle', \n",
    "           'boat',   # 10\n",
    "           'bus', \n",
    "           'car', \n",
    "           'motorbike', \n",
    "           'train', \n",
    "           'bottle', \n",
    "           'chair',  # 16\n",
    "           'diningtable', \n",
    "           'pottedplant', \n",
    "           'sofa', \n",
    "           'tvmonitor'\n",
    "           ]\n",
    "\n",
    "obj_class = ['background'] + obj_class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best=torch.load('weight/first_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"features.0.weight\", \"features.0.bias\", \"features.1.weight\", \"features.1.bias\", \"features.1.running_mean\", \"features.1.running_var\", \"features.1.num_batches_tracked\", \"features.3.weight\", \"features.3.bias\", \"features.4.weight\", \"features.4.bias\", \"features.4.running_mean\", \"features.4.running_var\", \"features.4.num_batches_tracked\", \"features.7.weight\", \"features.7.bias\", \"features.8.weight\", \"features.8.bias\", \"features.8.running_mean\", \"features.8.running_var\", \"features.8.num_batches_tracked\", \"features.10.weight\", \"features.10.bias\", \"features.11.weight\", \"features.11.bias\", \"features.11.running_mean\", \"features.11.running_var\", \"features.11.num_batches_tracked\", \"features.14.weight\", \"features.14.bias\", \"features.15.weight\", \"features.15.bias\", \"features.15.running_mean\", \"features.15.running_var\", \"features.15.num_batches_tracked\", \"features.17.weight\", \"features.17.bias\", \"features.18.weight\", \"features.18.bias\", \"features.18.running_mean\", \"features.18.running_var\", \"features.18.num_batches_tracked\", \"features.20.weight\", \"features.20.bias\", \"features.21.weight\", \"features.21.bias\", \"features.21.running_mean\", \"features.21.running_var\", \"features.21.num_batches_tracked\", \"features.24.weight\", \"features.24.bias\", \"features.25.weight\", \"features.25.bias\", \"features.25.running_mean\", \"features.25.running_var\", \"features.25.num_batches_tracked\", \"features.27.weight\", \"features.27.bias\", \"features.28.weight\", \"features.28.bias\", \"features.28.running_mean\", \"features.28.running_var\", \"features.28.num_batches_tracked\", \"features.30.weight\", \"features.30.bias\", \"features.31.weight\", \"features.31.bias\", \"features.31.running_mean\", \"features.31.running_var\", \"features.31.num_batches_tracked\", \"features.34.weight\", \"features.34.bias\", \"features.35.weight\", \"features.35.bias\", \"features.35.running_mean\", \"features.35.running_var\", \"features.35.num_batches_tracked\", \"features.37.weight\", \"features.37.bias\", \"features.38.weight\", \"features.38.bias\", \"features.38.running_mean\", \"features.38.running_var\", \"features.38.num_batches_tracked\", \"features.40.weight\", \"features.40.bias\", \"features.41.weight\", \"features.41.bias\", \"features.41.running_mean\", \"features.41.running_var\", \"features.41.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24084\\1759578980.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m                                 )   \n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mfinal_model_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mfinal_model_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mfinal_model_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20161699\\Anaconda3\\envs\\joowan\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1670\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1672\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"features.0.weight\", \"features.0.bias\", \"features.1.weight\", \"features.1.bias\", \"features.1.running_mean\", \"features.1.running_var\", \"features.1.num_batches_tracked\", \"features.3.weight\", \"features.3.bias\", \"features.4.weight\", \"features.4.bias\", \"features.4.running_mean\", \"features.4.running_var\", \"features.4.num_batches_tracked\", \"features.7.weight\", \"features.7.bias\", \"features.8.weight\", \"features.8.bias\", \"features.8.running_mean\", \"features.8.running_var\", \"features.8.num_batches_tracked\", \"features.10.weight\", \"features.10.bias\", \"features.11.weight\", \"features.11.bias\", \"features.11.running_mean\", \"features.11.running_var\", \"features.11.num_batches_tracked\", \"features.14.weight\", \"features.14.bias\", \"features.15.weight\", \"features.15.bias\", \"features.15.running_mean\", \"features.15.running_var\", \"features.15.num_batches_tracked\", \"features.17.weight\", \"features.17.bias\", \"features.18.weight\", \"features.18.bias\", \"features.18.running_mean\", \"features.18.running_var\", \"features.18.num_batches_tracked\", \"features.20.weight\", \"features.20.bias\", \"features.21.weight\", \"features.21.bias\", \"features.21.running_mean\", \"features.21.running_var\", \"features.21.num_batches_tracked\", \"features.24.weight\", \"features.24.bias\", \"features.25.weight\", \"features.25.bias\", \"features.25.running_mean\", \"features.25.running_var\", \"features.25.num_batches_tracked\", \"features.27.weight\", \"features.27.bias\", \"features.28.weight\", \"features.28.bias\", \"features.28.running_mean\", \"features.28.running_var\", \"features.28.num_batches_tracked\", \"features.30.weight\", \"features.30.bias\", \"features.31.weight\", \"features.31.bias\", \"features.31.running_mean\", \"features.31.running_var\", \"features.31.num_batches_tracked\", \"features.34.weight\", \"features.34.bias\", \"features.35.weight\", \"features.35.bias\", \"features.35.running_mean\", \"features.35.running_var\", \"features.35.num_batches_tracked\", \"features.37.weight\", \"features.37.bias\", \"features.38.weight\", \"features.38.bias\", \"features.38.running_mean\", \"features.38.running_var\", \"features.38.num_batches_tracked\", \"features.40.weight\", \"features.40.bias\", \"features.41.weight\", \"features.41.bias\", \"features.41.running_mean\", \"features.41.running_var\", \"features.41.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#   torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "#   print(\"using cuda:\", torch.cuda.get_device_name(0))\n",
    "#   pass\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# final_model_1=models.resnet18()\n",
    "\n",
    "\n",
    "# final_model_1.classifier=nn.Sequential(nn.Flatten(),\n",
    "#                                 nn.Linear(in_features=25088,out_features=4096,bias=True),\n",
    "#                                 nn.Dropout(0.5),\n",
    "#                                 nn.ReLU(inplace=True),\n",
    "                                \n",
    "#                                 nn.Linear(4096,1024,bias=True),\n",
    "#                                 nn.Dropout(0.5),\n",
    "#                                 nn.ReLU(inplace=True),\n",
    "                                \n",
    "#                                 nn.Linear(1024,21,bias=True)\n",
    "#                                 )   \n",
    "\n",
    "# final_model_1.load_state_dict(best['model_state_dict'])\n",
    "# final_model_1.to(device)\n",
    "# final_model_1.eval()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Linear(in_features=1024, out_features=21, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "# if yes, set default tensor type to cuda\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "  print(\"using cuda:\", torch.cuda.get_device_name(0))\n",
    "  pass\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "final_model_1=models.vgg16_bn(weights='VGG16_BN_Weights.DEFAULT')\n",
    "\n",
    "final_model_1.classifier=nn.Sequential(nn.Flatten(),\n",
    "                                nn.Linear(in_features=25088,out_features=4096,bias=True),\n",
    "                                nn.Dropout(0.5),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                \n",
    "                                nn.Linear(4096,1024,bias=True),\n",
    "                                nn.Dropout(0.5),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                \n",
    "                                nn.Linear(1024,21,bias=True)\n",
    "                                )   \n",
    "                                \n",
    "\n",
    "\n",
    "                 \n",
    "\n",
    "final_model_1.load_state_dict(best['model_state_dict'])\n",
    "final_model_1.to(device)\n",
    "final_model_1.eval()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idd='000001'\n",
    "\n",
    "img='VOC2007_test/JPEGImages/'+idd +'.jpg'\n",
    "anno='VOC2007_test/Annotations/'+idd+'.xml'\n",
    "image=cv2.imread(img)\n",
    "image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "ss.setBaseImage(image)\n",
    "ss.switchToSelectiveSearchFast()\n",
    "ssresults = ss.process()\n",
    "imout = image.copy()\n",
    "detect_boxes=[]\n",
    "box = []\n",
    "pred_score =[]\n",
    "gt_boxes=[]\n",
    "res=[]\n",
    "\n",
    "\n",
    "xml =  open(anno)\n",
    "tree = Et.parse(xml)\n",
    "root = tree.getroot()\n",
    "objects = root.findall(\"object\")\n",
    "\n",
    "for _object in objects:               # ground truth 정보 저장\n",
    "    bndbox = _object.find('bndbox')   # object -> bndbox 객체 반환\n",
    "    obj_name = _object.find('name').text   \n",
    "    xmin = int(bndbox.find('xmin').text)\n",
    "    xmax = int(bndbox.find('xmax').text)\n",
    "    ymin = int(bndbox.find('ymin').text)\n",
    "    ymax = int(bndbox.find('ymax').text)\n",
    "    idx=obj_class.index(obj_name)\n",
    "    gt_boxes.append([obj_name,idx,1,{'xmin':xmin,'ymin':ymin,'xmax':xmax,'ymax':ymax}])\n",
    "\n",
    "for e,result in enumerate(ssresults):\n",
    "    if e < 500:\n",
    "        x,y,w,h = result\n",
    "        timage = imout[y:y+h,x:x+w]\n",
    "        cropped_arround_img= around_context(imout, x,y,w,h,16)\n",
    "        cropped_arround_img = np.array(cropped_arround_img)\n",
    "        resized = cv2.resize(cropped_arround_img, (224,224), interpolation = cv2.INTER_AREA)\n",
    "        resized=resized/255.0\n",
    "        resized = np.expand_dims(resized, axis=0)\n",
    "        resized=np.transpose(resized,(0,3,1,2))\n",
    "        input=torch.FloatTensor(resized).to(device)\n",
    "        pred=final_model_1(input)[0].cpu().detach().numpy()\n",
    "        \n",
    "        print(pred)\n",
    "        #softmax implementation\n",
    "        maxi=np.max(pred)\n",
    "        scores = pred - maxi\n",
    "        e_score = np.exp(scores)\n",
    "        e_sum = np.sum(e_score)\n",
    "        sm = e_score / e_sum\n",
    "\n",
    "        # confidence score\n",
    "        idx=np.argmax(sm)\n",
    "        \n",
    "\n",
    "        if idx==0:\n",
    "            continue\n",
    "        if sm[idx]<0.98:\n",
    "            continue\n",
    "\n",
    "        confidence_score=sm[idx]\n",
    "        \n",
    "        if w<50 :\n",
    "            continue\n",
    "        if h<50:\n",
    "            continue\n",
    "        \n",
    "        pred_score.append(sm)\n",
    "        box.append({'xmin':x,'ymin':y,'xmax':x+w,'ymax':y+h})\n",
    "        filename=obj_class[idx]\n",
    "        detect_boxes.append([filename,idx,sm[idx],{'xmin':x,'ymin':y,'xmax':x+w,'ymax':y+h}])\n",
    "        res.append([filename,confidence_score,x,y,x+w,y+h])\n",
    "\n",
    "\n",
    "predict = np.array(list(map(lambda x: obj_class[np.argmax(x)], pred_score)))\n",
    "box = np.array(box)\n",
    "pred_score = np.array(pred_score)\n",
    "pred_score_max = list(map(lambda x: max(x), pred_score))\n",
    "\n",
    "NMS_idx = non_max_suppression(box,pred_score,overlapThresh = 0.1, class_list = obj_class)\n",
    "final_boxes=[]\n",
    "for k in NMS_idx:\n",
    "    filename,confidence_score,xmin,ymin,xmax,ymax=res[k]\n",
    "    image_rec = cv2.rectangle(imout,( xmin,ymin,xmax,ymax),(255,0,0), 2)\n",
    "    cv2.putText(image_rec, filename+ ':' +confidence_score.round(2).astype(str),(int(xmin/2+xmax/2),int(ymin)+30 ), cv2.FONT_HERSHEY_SIMPLEX,0.9,(255,0,0),2)\n",
    "    final_boxes.append(detect_boxes[k])\n",
    "\n",
    "\n",
    "\n",
    "plt.imshow(imout)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=cv2.imread(img)\n",
    "image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "imout=image.copy()\n",
    "\n",
    "\n",
    "xml =  open(anno)\n",
    "tree = Et.parse(xml)\n",
    "root = tree.getroot()\n",
    "objects = root.findall(\"object\")\n",
    "name=[]\n",
    "\n",
    "for _object in objects:               # ground truth 정보 저장\n",
    "    bndbox = _object.find('bndbox')   # object -> bndbox 객체 반환\n",
    "    obj_name = _object.find('name').text   \n",
    "    xmin = int(bndbox.find('xmin').text)\n",
    "    xmax = int(bndbox.find('xmax').text)\n",
    "    ymin = int(bndbox.find('ymin').text)\n",
    "    ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "    image_rec = cv2.rectangle(imout,( xmin,ymin,xmax,ymax),(255,0,0), 2)\n",
    "    cv2.putText(image_rec,obj_name,(int(xmin/2+xmax/2),int(ymin)+30 ), cv2.FONT_HERSHEY_SIMPLEX,0.9,(255,0,0),2)\n",
    "    name.append(obj_name)\n",
    "\n",
    "plt.imshow(imout)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=[]\n",
    "for i in NMS_idx:\n",
    "    if res[i][0] in name:\n",
    "        idx.append(i)\n",
    "\n",
    "image=cv2.imread(img)\n",
    "image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "imout=image.copy()\n",
    "\n",
    "for k in idx:\n",
    "    filename,confidence_score,xmin,ymin,xmax,ymax=res[k]\n",
    "    image_rec = cv2.rectangle(imout,( xmin,ymin,xmax,ymax),(255,0,0), 2)\n",
    "    cv2.putText(image_rec, filename+ ':' +confidence_score.round(2).astype(str),(int(xmin/2+xmax/2-30),int(ymin)+30 ), cv2.FONT_HERSHEY_SIMPLEX,0.7,(255,0,0),2)\n",
    "\n",
    "plt.imshow(imout)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "annot_path='VOC2007_test/Annotations'      # annotaion data가 있는 경로\n",
    "img_path='VOC2007_test/JPEGImages'      # 이미지 데이터가 있는 경로\n",
    "count=0\n",
    "state=0\n",
    "pred_boxes=[]\n",
    "true_boxes=[]\n",
    "\n",
    "\n",
    "for e1,i in enumerate(tqdm(os.listdir(annot_path),desc=' ')):\n",
    "    \n",
    "    \n",
    "    file_name=i.split('.')[0]\n",
    "    img_file=i.split('.')[0]+'.jpg'\n",
    "    print(f'{img_path}/{img_file} ready')\n",
    "\n",
    "    xml =  open(os.path.join(annot_path,i),'r')\n",
    "    tree = Et.parse(xml)\n",
    "    root = tree.getroot()\n",
    "    objects = root.findall(\"object\")\n",
    "\n",
    "    for _object in objects:               # ground truth 정보 저장\n",
    "        bndbox = _object.find('bndbox')   # object -> bndbox 객체 반환\n",
    "        obj_name = _object.find('name').text   \n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        idx=obj_class.index(obj_name)\n",
    "        true_boxes.append([e1,idx,1,{'xmin':xmin,'ymin':ymin,'xmax':xmax,'ymax':ymax}])\n",
    "\n",
    "    \n",
    "    img=img_path+'/'+img_file\n",
    "    image=cv2.imread(img)\n",
    "    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    \n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(image)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    ssresults = ss.process()\n",
    "    imout = image.copy()\n",
    "    box = []\n",
    "    detect_box=[]\n",
    "    pred_score =[]\n",
    "\n",
    "    for e2,result in enumerate(ssresults):\n",
    "        if e2 < 500:\n",
    "            x,y,w,h = result\n",
    "            timage = imout[y:y+h,x:x+w]\n",
    "            cropped_arround_img= around_context(imout, x,y,w,h,16)\n",
    "            cropped_arround_img = np.array(cropped_arround_img)\n",
    "            resized = cv2.resize(cropped_arround_img, (224,224), interpolation = cv2.INTER_AREA)\n",
    "            resized=resized/255.0\n",
    "            resized = np.expand_dims(resized, axis=0)\n",
    "            resized=np.transpose(resized,(0,3,1,2))\n",
    "            input=torch.FloatTensor(resized).to(device)\n",
    "            pred=final_model_1(input)[0].cpu().detach().numpy()\n",
    "        \n",
    "            #softmax implementation\n",
    "            maxi=np.max(pred)\n",
    "            scores = pred - maxi\n",
    "            e_score = np.exp(scores)\n",
    "            e_sum = np.sum(e_score)\n",
    "            sm = e_score / e_sum\n",
    "\n",
    "            # confidence score\n",
    "            idx=np.argmax(sm)\n",
    "            \n",
    "\n",
    "            if idx==0:\n",
    "                continue\n",
    "            if sm[idx]<0.50:\n",
    "                continue\n",
    "            \n",
    "            confidence_score=sm[idx]\n",
    "\n",
    "            if w<50 :\n",
    "                continue\n",
    "            if h<50:\n",
    "                continue\n",
    "            \n",
    "            pred_score.append(sm)\n",
    "            filename=obj_class[idx]\n",
    "            box.append({'xmin':x,'ymin':y,'xmax':x+w,'ymax':y+h})\n",
    "            detect_box.append([e1,idx,sm[idx],{'xmin':x,'ymin':y,'xmax':x+w,'ymax':y+h}])\n",
    "    \n",
    "    #predict = np.array(list(map(lambda x: obj_class[np.argmax(x)], pred_score)))\n",
    "    box = np.array(box)\n",
    "    pred_score = np.array(pred_score)\n",
    "    NMS_idx = non_max_suppression(box,pred_score,overlapThresh = 0.2, class_list = obj_class)\n",
    "    \n",
    "    for k in NMS_idx:\n",
    "        pred_boxes.append(detect_box[k])\n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map,ap_class=mean_average_precision(pred_boxes,true_boxes,obj_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{map:.4f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joowan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b94245a966602fa1d3337cb3b1178b7109c129cd4eabde98685205c86aedb4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
